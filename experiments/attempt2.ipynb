{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MxfZWnLvif6L",
        "outputId": "92c8cc43-96c1-4f7c-cabc-f99f735dc9f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing collected packages: huggingface_hub, tokenizers\n",
            "Successfully installed huggingface_hub-0.17.3 tokenizers-0.14.1\n"
          ]
        }
      ],
      "source": [
        "# !git clone https://www.github.com/ivanrj7j/NLP2\n",
        "# !pip install tokenizers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Checking Data"
      ],
      "metadata": {
        "id": "jWwB_H9ok1WK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from DataLoader import TrainLoader\n",
        "from Tokenizer import NLPTokenizer"
      ],
      "metadata": {
        "id": "tR7jg-EviuSJ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import keras\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "TEWeozGmlVWI"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import Tokenizer as t"
      ],
      "metadata": {
        "id": "jspf_oYc0bh1"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Bidirectional, LSTM, Embedding, Dense, Dropout\n",
        "from keras.models import Sequential, Model\n",
        "from keras.callbacks import EarlyStopping"
      ],
      "metadata": {
        "id": "353sDvlElWVu"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainDataset = TrainLoader(1500, 25, \"/content/NLP2/data/tokenizer/tokens.json\", \"/content/NLP2/data/preprocessed/valid.csv\",  100)\n",
        "testDataset = TrainLoader(1500, 25, \"/content/NLP2/data/tokenizer/tokens.json\", \"/content/NLP2/data/preprocessed/_test.csv\",  100)"
      ],
      "metadata": {
        "id": "c0wagqbmi4fJ"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Trained using valid data to instead of whole training data"
      ],
      "metadata": {
        "id": "wPFWuo2Gk3uP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for x,y in testDataset:\n",
        "  x = testDataset.tokenizer.decode(x)\n",
        "  y = testDataset.tokenizer.decode(y)\n",
        "\n",
        "  data = zip(x, y)\n",
        "  for _x, _y in data:\n",
        "    print(_x, '\\t\\t', _y)\n",
        "  break"
      ],
      "metadata": {
        "id": "zH80c7M_jcza"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checking the testDataset loader"
      ],
      "metadata": {
        "id": "-L7n-wsBlIWI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining and training model"
      ],
      "metadata": {
        "id": "3rzY8onDlPx_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential([\n",
        "    Embedding(30003, 1000),\n",
        "    Dropout(0.8),\n",
        "    Bidirectional(LSTM(100)),\n",
        "    Dropout(0.4),\n",
        "    Dense(30003, \"softmax\")\n",
        "    ])"
      ],
      "metadata": {
        "id": "gntYCwXmlMHH"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\"adam\", \"categorical_crossentropy\", \"accuracy\")\n",
        "model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QomxqGIJlgjO",
        "outputId": "f2643e9a-bea4-4676-b0d0-52fd593ecc5b"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, None, 1000)        30003000  \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, None, 1000)        0         \n",
            "                                                                 \n",
            " bidirectional_1 (Bidirecti  (None, 200)               880800    \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 200)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 30003)             6030603   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 36914403 (140.82 MB)\n",
            "Trainable params: 36914403 (140.82 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "earlyStopper = EarlyStopping(patience=3)"
      ],
      "metadata": {
        "id": "ho1yMWdQlijp"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(trainDataset.getTensorflowDataset(), epochs=50, callbacks=[earlyStopper], validation_data=testDataset.getTensorflowDataset())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ehRQJVXVlkta",
        "outputId": "41f708bd-0cb1-403c-dc6a-ee6855017ab7"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "440/440 [==============================] - 128s 281ms/step - loss: 7.1969 - accuracy: 0.0846 - val_loss: 6.7304 - val_accuracy: 0.1319\n",
            "Epoch 2/50\n",
            "440/440 [==============================] - 113s 256ms/step - loss: 6.5328 - accuracy: 0.1421 - val_loss: 6.3712 - val_accuracy: 0.1613\n",
            "Epoch 3/50\n",
            "440/440 [==============================] - 117s 266ms/step - loss: 6.2308 - accuracy: 0.1660 - val_loss: 6.1411 - val_accuracy: 0.1813\n",
            "Epoch 4/50\n",
            "440/440 [==============================] - 115s 260ms/step - loss: 6.0144 - accuracy: 0.1800 - val_loss: 5.9892 - val_accuracy: 0.1920\n",
            "Epoch 5/50\n",
            "440/440 [==============================] - 115s 261ms/step - loss: 5.8523 - accuracy: 0.1891 - val_loss: 5.8882 - val_accuracy: 0.1993\n",
            "Epoch 6/50\n",
            "440/440 [==============================] - 114s 258ms/step - loss: 5.7224 - accuracy: 0.1960 - val_loss: 5.8161 - val_accuracy: 0.2044\n",
            "Epoch 7/50\n",
            "440/440 [==============================] - 114s 260ms/step - loss: 5.6144 - accuracy: 0.2015 - val_loss: 5.7508 - val_accuracy: 0.2097\n",
            "Epoch 8/50\n",
            "440/440 [==============================] - 117s 265ms/step - loss: 5.5196 - accuracy: 0.2064 - val_loss: 5.7022 - val_accuracy: 0.2129\n",
            "Epoch 9/50\n",
            "440/440 [==============================] - 114s 258ms/step - loss: 5.4337 - accuracy: 0.2105 - val_loss: 5.6687 - val_accuracy: 0.2159\n",
            "Epoch 10/50\n",
            "440/440 [==============================] - 115s 261ms/step - loss: 5.3615 - accuracy: 0.2140 - val_loss: 5.6455 - val_accuracy: 0.2185\n",
            "Epoch 11/50\n",
            "440/440 [==============================] - 115s 262ms/step - loss: 5.2910 - accuracy: 0.2172 - val_loss: 5.6228 - val_accuracy: 0.2207\n",
            "Epoch 12/50\n",
            "440/440 [==============================] - 114s 258ms/step - loss: 5.2264 - accuracy: 0.2199 - val_loss: 5.6050 - val_accuracy: 0.2228\n",
            "Epoch 13/50\n",
            "440/440 [==============================] - 114s 259ms/step - loss: 5.1720 - accuracy: 0.2225 - val_loss: 5.5911 - val_accuracy: 0.2242\n",
            "Epoch 14/50\n",
            "440/440 [==============================] - 115s 260ms/step - loss: 5.1227 - accuracy: 0.2252 - val_loss: 5.5873 - val_accuracy: 0.2266\n",
            "Epoch 15/50\n",
            "440/440 [==============================] - 114s 258ms/step - loss: 5.0826 - accuracy: 0.2278 - val_loss: 5.5746 - val_accuracy: 0.2285\n",
            "Epoch 16/50\n",
            "440/440 [==============================] - 112s 255ms/step - loss: 5.0504 - accuracy: 0.2290 - val_loss: 5.5729 - val_accuracy: 0.2294\n",
            "Epoch 17/50\n",
            "440/440 [==============================] - 116s 265ms/step - loss: 5.0105 - accuracy: 0.2310 - val_loss: 5.5683 - val_accuracy: 0.2312\n",
            "Epoch 18/50\n",
            "440/440 [==============================] - 114s 258ms/step - loss: 4.9939 - accuracy: 0.2328 - val_loss: 5.5634 - val_accuracy: 0.2321\n",
            "Epoch 19/50\n",
            "440/440 [==============================] - 114s 260ms/step - loss: 4.9459 - accuracy: 0.2346 - val_loss: 5.5597 - val_accuracy: 0.2330\n",
            "Epoch 20/50\n",
            "440/440 [==============================] - 113s 258ms/step - loss: 4.9061 - accuracy: 0.2369 - val_loss: 5.5726 - val_accuracy: 0.2338\n",
            "Epoch 21/50\n",
            "440/440 [==============================] - 113s 258ms/step - loss: 4.8857 - accuracy: 0.2383 - val_loss: 5.5580 - val_accuracy: 0.2348\n",
            "Epoch 22/50\n",
            "440/440 [==============================] - 116s 265ms/step - loss: 4.8503 - accuracy: 0.2397 - val_loss: 5.5527 - val_accuracy: 0.2362\n",
            "Epoch 23/50\n",
            "440/440 [==============================] - 114s 259ms/step - loss: 4.8268 - accuracy: 0.2416 - val_loss: 5.5547 - val_accuracy: 0.2376\n",
            "Epoch 24/50\n",
            "440/440 [==============================] - 117s 265ms/step - loss: 4.7959 - accuracy: 0.2431 - val_loss: 5.5629 - val_accuracy: 0.2378\n",
            "Epoch 25/50\n",
            "440/440 [==============================] - 116s 264ms/step - loss: 4.7786 - accuracy: 0.2438 - val_loss: 5.5608 - val_accuracy: 0.2388\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"/content/drive/MyDrive/NLP Models/bidirectional2.keras\")"
      ],
      "metadata": {
        "id": "P-fKeT13lnB-"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = t.from_file(\"/content/NLP2/data/tokenizer/tokens.json\")"
      ],
      "metadata": {
        "id": "OH0ZT8UY0K03"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepareInput(text:str) -> np.ndarray:\n",
        "  tokens = np.array(tokenizer.encode(text).ids)\n",
        "\n",
        "  if tokens.shape[0] < 25:\n",
        "    tokens = np.pad(tokens, 25-tokens.shape[0], \"constant\")\n",
        "\n",
        "  return tokens[-25:]"
      ],
      "metadata": {
        "id": "A2qE0UX40G9A"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predictText(start:str, limit:int=100):\n",
        "  tokens = prepareInput(start)\n",
        "  sentence = start\n",
        "  for _ in range(limit):\n",
        "    prediction = model.predict(prepareInput(\"Media\").reshape(1, -1), verbose=False).argmax(axis=1)\n",
        "    tokens = np.hstack((tokens, prediction))[-25:]\n",
        "    sentence += \" \" + tokenizer.decode(prediction).replace('##', \"\")\n",
        "    print(sentence, end=\"\\r\")\n",
        "  print(tokens)\n",
        "  return sentence"
      ],
      "metadata": {
        "id": "NqF1jTJD0HNp"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictText(\"India is the nation\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "m5WYxz3p0I-J",
        "outputId": "e6c88f50-c475-407c-8044-e926cd095b66"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[420 420 420 420 420 420 420 420 420 420 420 420 420 420 420 420 420 420\n",
            " 420 420 420 420 420 420 420]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'India is the nation &# &# &# &# &# &# &# &# &# &# &# &# &# &# &# &# &# &# &# &# &# &# &# &# &# &# &# &# &# &# &# &# &# &# &# &# &# &# &# &# &# &# &# &# &# &# &# &# &# &# &# &# &# &# &# &# &# &# &# &# &# &# &# &# &# &# &# &# &# &# &# &# &# &# &# &# &# &# &# &# &# &# &# &# &# &# &# &# &# &# &# &# &# &# &# &# &# &# &# &#'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Failure! now i will try a different approach to this problem, I was avoiding to remove stop words and punctuations till now, now i am reconsidering. I will be removing stop words from tokenizer and make some other more changes!"
      ],
      "metadata": {
        "id": "IdjakTbx0tLH"
      }
    }
  ]
}